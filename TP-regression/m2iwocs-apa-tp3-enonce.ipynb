{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3 \"Régression polynomiale / Régularisation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*M2 IWOCS, Apprentissage automatique*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données du problème\n",
    "Télécharger l’archive correspondant aux données de l’exercice 3/TP 3, décompactez-la. cette archive contient deux fichiers, un pour les données d'entrée x qui sont ici à une dimension, et l’autre pour les données de sorties y à une dimension également. Tracer un graphique représentant le nuage de points défini par ces données dans le style du celui qui est représenté dans la figure suivante.\n",
    "<img src=\"tp3-enonce-fig1.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.12.3)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"/home/amine-cheikh/Bureau/M2/App auto/TP-regression/venv/bin/python\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "x = np.loadtxt('ex3dat/ex3Linx')\n",
    "y = np.loadtxt('ex3dat/ex3Liny')\n",
    "\n",
    "print(\"x: \\n\",x)\n",
    "print(\"y: \\n\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse du problème\n",
    "On propose de modéliser ces données par une fonction hypothèse polynomiale de degré 5 :\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + + \\theta_3 x^3 + \\theta_4 x^4 + \\theta_5 x^5\n",
    "$$\n",
    "\n",
    "Comme expliqué dans le cours, cela revient à considérer 6 descripteurs constituant les différentes puissances de x dans le polynôme. Le problème reste cependant linéaire par rapport à ces descripteurs.\n",
    "\n",
    "Le degré du polynôme étant 5 et le nombre de points initiaux 7, le risque de sur-apprentissage (over-fitting) est grand. Il est donc nécessaire de *régulariser* le modèle.\n",
    "\n",
    "Pour cela on va corriger le calcul de la fonction de coût en utilisant une variante de la régularisation de Ridge vu en cours et qui s'écrit :\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}\\left(h_\\theta(x_i)-y_i\\right)^2+\\lambda \\sum_{j=1}^{n}\\theta_j^2\n",
    "$$\n",
    "\n",
    "Le paramètre de régularisation $\\lambda$ va permettre d’éviter des valeurs importantes de $\\theta$ qui feraient augmenter la valeur de la fonction de coût. A noter que la composantes $\\theta_O$ n’intervient pas dans la sommation devant $\\lambda$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification des formulations matricielles pour intégrer le terme de régularisation de type Ridge\n",
    "L'introduction de la régularisation intervient dans la fonction de calcul du coût et dans le calcul de la minimisation de la fonction coût avec la méthode de gradient\n",
    "\n",
    "### Calcul du coût\n",
    "\n",
    "Formule du coût avec la régularisation proposée dans l'exercice (variante de la régularisation de Ridge) :\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}\\left(h_\\theta(x_i)-y_i\\right)^2+\\lambda \\sum_{j=1}^{n}\\theta_j^2\n",
    "$$\n",
    "\n",
    "Comme vu précédemment, la première partie de cette formule (avant le terme de régularisation)\n",
    "$$\n",
    "\\frac{1}{2m}\\sum_{i=1}^{m}\\left(h_\\theta(x_i)-y_i\\right)^2\n",
    "$$\n",
    "s'écrit en notation matricielle\n",
    "$$\n",
    "\\frac{1}{2m}\\left[h_\\theta(X)-Y\\right]^t \\left[h_\\theta(X)-Y\\right]\n",
    "$$\n",
    "avec\n",
    "\\begin{align}\n",
    " \\text{avec} \\quad\n",
    " h_\\theta(X) &= X \\theta\\\\\n",
    "             &= \n",
    "             \\begin{bmatrix}\n",
    "             1 & x_{11} & \\cdots & x_{1n}\\\\\n",
    "             \\vdots & \\vdots & & \\vdots\\\\\n",
    "             1 & x_{m1} & \\cdots & x_{mn}\\\\\n",
    "             \\end{bmatrix}\n",
    "             \\begin{bmatrix}\n",
    "             \\theta_0\\\\\n",
    "             \\vdots\\\\\n",
    "             \\theta_n\n",
    "             \\end{bmatrix}\\\\\n",
    "\\text{et} \\quad \n",
    "Y &= \\begin{bmatrix}\n",
    "      y_1\\\\\n",
    "      \\vdots\\\\\n",
    "      y_m\n",
    "      \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "On doit alors ajouter à la fonction coût, le terme $\\lambda \\sum_{j=1}^{n}\\theta_j^2$ (attention : on a exclu $\\theta_0$). En notation matricielle, cette expression s'écrit\n",
    "\n",
    "$$\n",
    "\\lambda [\\theta_-]^t[\\theta_-] \\quad \\text{où} \\quad \n",
    "\\theta_- = \\begin{bmatrix}\n",
    "                0\\\\\n",
    "                \\theta_1\\\\\n",
    "                \\vdots\\\\\n",
    "                \\theta_n\n",
    "            \\end{bmatrix}\n",
    "$$\n",
    "Donc au final $J(\\theta)$ s'écrit en notation matricielle\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m}\\left[X \\theta -Y \\right]^t \\left[X \\theta - Y\\right] + \\lambda [\\theta_-]^t[\\theta_-]\n",
    "$$\n",
    "\n",
    "###  Calcul de la minimisation de la fonction de coût régularisée avec la méthode de gradiant\n",
    "\n",
    "A chaque itération, on calcule une nouvelle valeur de $\\theta^*$ à partir de $\\theta$ telle que $J(\\theta^*) < J(\\theta)$.\n",
    "\n",
    "La méthode de gradient correspond au calcul suivant :\n",
    "$$\n",
    "\\theta^* = \\theta - \\alpha \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "\\begin{cases}\n",
    "\\theta_0^* &= \\theta_0 - \\alpha \\displaystyle{\\frac{\\partial}{\\partial \\theta_0}} J(\\theta)\\\\\n",
    "\\vdots\\\\\n",
    "\\theta_k^* &= \\theta_k - \\alpha \\displaystyle{\\frac{\\partial}{\\partial \\theta_k}} J(\\theta)\\\\\n",
    "\\vdots\\\\\n",
    "\\theta_k^* &= \\theta_k - \\alpha \\displaystyle{\\frac{\\partial}{\\partial \\theta_k}} J(\\theta)\n",
    "\\end{cases}\n",
    "\n",
    "\\begin{align}\n",
    "\\displaystyle{\\frac{\\partial}{\\partial \\theta_k}} J(\\theta) &= \n",
    "    \\displaystyle{\\frac{\\partial}{\\partial \\theta_k}} \n",
    "        \\left( \\frac{1}{2m}\\sum_{i=1}^{m}\\left(h_\\theta(x_i)-y_i\\right)^2+\\lambda \\sum_{j=1}^{n}\\theta_j^2 \\right)\\\\\n",
    "     &= \\frac{1}{m}\\left(\\sum_{i=1}^{m}\\left(h_\\theta(x_i)-y_i\\right)x_{ik}\\right) + 2 \\lambda \\theta_k\n",
    "\\end{align}\n",
    "Le dernier terme du membre droit de l'équation $2 \\lambda \\theta_k$ vaut 0 si $k=0$ en raison de la formule de régularisation utilisée.\n",
    "\n",
    "La formulation matricielle s'écrit ainsi (en utilisant que $h_\\theta(X) = X \\theta$) :\n",
    "$$\n",
    "\\theta^* = \\theta - \\alpha \\left[ \\frac{1}{m} X^t [X \\theta - Y] + 2 \\lambda \\theta_- \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Discussion sur le choix de la régularisation de type Ridge utilisée\n",
    "Afin de comprendre l'intérêt du choix de la régularisation utilisée ici, à savoir une régularisation Ridge sans prendre le premier terme $\\theta_0$, on pourra comparer les résultats que l'on obtiendra dans la mise en ouvre proposée ci-dessous avec une régularisation de Ridge complète et intégrant $\\theta_0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en oeuvre\n",
    "\n",
    "Il s’agit d’adapter l’algorithme de régression linéaire écrite sous forme matricielle et modifié à partir des formules précédents pour intégrer le terme de régularisation. \n",
    "\n",
    "Comme cela est expliqué dans le cours, on va ramener le problème de regression polynomiale univarié à un problème de régression linéaire multivarié, en constituant une matrice X de la façon suivante à partir du vecteur colonne x de la donnée initiale :\n",
    "\n",
    "\n",
    "`X=np.block([[xone,x,x**2,x**3,x**4,x**5]])`\n",
    "\n",
    "Réaliser cette régression pour $\\lambda = 0, \\lambda = 1, \\lambda = 10$, puis tracer la courbe résultante sur le même graphique que les données. On doit alors obtenir une figure comparable à la suivante :\n",
    "\n",
    "<img src=\"tp3-enonce-fig2.png\" width=\"900\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,Y, alpha = 0.07, nb_iterations_max = 2500, e = 1e-6): \n",
    "    # Initialisation \n",
    "    theta = np.zeros(X.shape[1])\n",
    "        \n",
    "    for i in range(nb_iterations_max): \n",
    "        theta_old = theta.copy()    # on copie la valeur de theta \n",
    "        \n",
    "        theta = iteration(theta,X,Y,alpha) # calcule de la nouvelle valeur optimal du vecteur theta\n",
    "        \n",
    "        # Afficher le progrès\n",
    "        if i % 100 == 0:  \n",
    "            print(f\"Itération {i:4d}: Theta = {theta}\")\n",
    "            \n",
    "        # Critère d'arrets (différence relative)\n",
    "        if i > 0: # condition pour eviter les division par zéro à la première itération \n",
    "           \n",
    "           # Calcule |θ^(n+1) - θ^(n)| \n",
    "           diff_theta = np.linalg.norm(theta-theta_old)\n",
    "           \n",
    "           #Calcule de theta |θ^(n)|\n",
    "           norm_theta_old = np.linalg.norm(theta_old)\n",
    "            \n",
    "            # Éviter la division par zéro  \n",
    "           if norm_theta_old > 1e-10: \n",
    "            # Calcule du critère d'arrêt \n",
    "              diff_rel = diff_theta / norm_theta_old\n",
    "              \n",
    "              # Vérifier le critère d'arrêt\n",
    "              if diff_rel < e :\n",
    "                  print(f\"Critère d'arrêt déclencher ! Fin de la boucle\") \n",
    "                  break\n",
    "            \n",
    "    else : \n",
    "        print(\"Nombre d'itération maximum atteint ! \")\n",
    "                   \n",
    "    print(f\"Theta optimal :{theta}\")\n",
    "    \n",
    "    return theta      \n",
    "            \n",
    "# Utilisation \n",
    "theta_optimal = gradient(X,y,alpha = 0.07, e=1e-6)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compromis biais-variance\n",
    "\n",
    "A partir de l’article wikipedia sur le compromis biais-variance en apprentissage : [https://fr.wikipedia.org/wiki/Dilemme_biais-variance](https://fr.wikipedia.org/wiki/Dilemme_biais-variance) qui est référencé dans le cours, déduire une valeur de $\\lambda$ qui réalise le bon compromis entre biais du modèle et variance du modèle. On pourra faire les mêmes calculs que précédemment avec chaque valeur $\\lambda$  du vecteur de valeurs $\\lambda  = [0,0.5,1,...,5]$ et obtenir une série de courbes comme décrites ci-dessous :\n",
    "\n",
    "<img src=\"tp3-enonce-fig3.png\" width=\"600\"/>\n",
    "\n",
    "Tracer les courbes de calcul du biais et de la variance du modèle pour chacune de ces valeurs et analyser le résultat pour conclure.\n",
    "\n",
    "Rappel de la formule de décomposition de l’erreur décrite dans le cours :\n",
    "\n",
    "\n",
    "- le biais se calcule par \n",
    "\n",
    "$$\n",
    "B(h(x))= E[(h-f)(x)]\n",
    "$$ \n",
    "\n",
    "- la variance se calcule par \n",
    "\n",
    "$$\n",
    "Var(h(x))=E[h^2(x)]-(E[h(x)])^2\n",
    "$$\n",
    "\n",
    "On définit ensuite l'*erreur de prédiction attendue* par\n",
    "\n",
    "$$\n",
    "Err(h(x)) = B^2(h(x)) + Var(h(x)) + \\sigma^2\n",
    "$$\n",
    "\n",
    "où $E$ correspondra ici à l’espérance mathématique sur l’ensemble des exemples d’apprentissage utilisé. \n",
    "\n",
    "L’intersection des courbes pour ces deux grandeurs en fonction de $\\lambda$ ainsi que l’erreur globale indiquera la valeur du compromis biais-variance.\n",
    "\n",
    "**Remarque** : Comme précisé dans le cours, l’analyse biais-variance se fait généralement sur plusieurs échantillonages ou jeux de données x (on en a qu’un seul ici car il y a trop peu de points expérimentaux) à partir desquels on obtient autant de fonctions hypothèses sur lesquels on doit moyenner les résultats. Le cas présenté est donc trop réduit pour cette analyse. Le résultat obtenu est de ce fait peu précis, voire peu fiable ici.\n",
    "\n",
    "<img src=\"tp3-enonce-fig4.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Éléments de calcul du compromis biais-variance proposé\n",
    "\n",
    "Le Biais défini par la formule\n",
    "\n",
    "$$\n",
    "B(h(x))= E[(h-f)(x)]\n",
    "$$\n",
    "\n",
    "se calcule en faisant la moyenne (l'espérance se simplifie en moyenne, comme expliqué dans le cours) des coefficients du vecteur $y-h(x)$. Ce calcul s'obtient en appelant la fonction `mean`de `numpy`, à laquelle on passera en paramètre le vecteur/matrice $y-h(x)$.\n",
    "\n",
    "La variance se calcule par la formule \n",
    "$$\n",
    "Var[h(x)] = E[(h(x))^2] - (E[h(x)])^2\n",
    "$$\n",
    "\n",
    "Là encore on simplifie l'espérance mathématique $E$ par un simple calcul de moyenne. \n",
    "\n",
    "L'expression $Var[h(x)]$ se calculera simplement en appelant la fonction `var`de `numpy`, à laquelle on passera en paramètre le vecteur/matrice $h(x)$. \n",
    "\n",
    "Comme cela est mentionné, ce calcul de variance est une \"interprétation\" simplifiée de la définition du cours qui nécessite de mesurer la dispersion des valeurs de $h(x)$ par rapport à un ensemble d'échantillonages que l'on ne fait pas ici dans le TP. Calculer la \"vraie\" variance nécessiterait de mettre en place cet échantillonage et compliquerait singulièrement le TP. L'\"interprétation\" fait pour cet exercice est simple à calculer et produit des résultats exploitables et semblables au comportement de la \"vraie\" variance pour les données fournies.\n",
    "\n",
    "Les courbes du biais et de la variance sont tracées dans cet exercice en prenant en abscisses les valeurs de $\\lambda$. Mais comme la \"complexité\" du modèle est plus grande lorsque $\\lambda$ est petit, les sens de variations des courbes seront à l'inverse de la figure donnée en cours. Ici la courbe du biais va croitre alors que celle de la variance va décroitre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprise du développement en utilisant la bibliothèque `scikit-learn`\n",
    "Reprendre le programme en Python en remplaçant les fonctions de calcul de régression par les fonctions prédéfinies de la bibliothèse `scikit-learn` qui implémente les techniques de régularisation connues dont celle de Ridge sur laquelle s'appuie la méthode proposée dans le TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
